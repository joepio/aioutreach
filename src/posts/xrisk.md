---
title: The existential risk of superintelligent AI
description: Why AI is a risk for the future of our existence, and why we need to pause development.
---

## AI is getting smarter

The field of AI has been making huge progress in the last years.
We went from awkward next-word prediction mistakes in our smartphone keyboards to [GPT-4 beating 90% of lawyers at the bar exam](https://www.forbes.com/sites/johnkoetsier/2023/03/14/gpt-4-beats-90-of-lawyers-trying-to-pass-the-bar/?sh=5747c29f3027).
Powerful articifial intelligence brings amazing opportunities, but also huge risks.

## Experts are sounding the alarm

[Over half of AI researchers surveyed think thereâ€™s a larger than 10% risk](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) that once we build a superintelligent AI, it will **destroy all life on earth**.
When you ask AI safety researchers (the people who actually research the safety risks of AI), this number [grows to 30%](https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results).

The [letter for pausing AI development](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), launched in april 2023, has been signed over 27,000 times, mostly by AI researchers and tech leaders.

The list includes people like:

- **Stuart Russell**, writer of the #1 textbook on Artificial Intelligence used in most AI studies
- **Geoffrey Hinton**, the "Godfather of AI", [left Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) to warn people of AI.
- **Yoshua Bengio**, deep learning pioneer and winner of the Turing Award.
- **Elon Musk**, co-founder of OpenAI, SpaceX and Tesla: ["AI has the potential of civilizational destruction"](https://www.inc.com/ben-sherry/elon-musk-ai-has-the-potential-of-civilizational-destruction.html)

But this is not the only time that we've been warned about the existential dangers of AI:

- **Stephen Hawking**, theoretical physicist & cosmologist: ["The development of full artificial intelligence could spell the end of the human race"](https://nypost.com/2023/05/01/stephen-hawking-warned-ai-could-mean-the-end-of-the-human-race/).
- **Eliezer Yudkowsky**, founder of MIRI and conceptual father of the AI safety field: ["If we go ahead on this everyone will die"](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/).
- **Bill Gates** warned that ["AI could decide that humans are a threat"](https://www.denisonforum.org/daily-article/bill-gates-ai-humans-threat/).
- **Sam Altman** (yes, the CEO of OpenAI who builds ChatGPT): ["Why you should fear machine intelligence"](https://blog.samaltman.com/machine-intelligence-part-1).

## Why superintelligence is dangerous

Intelligence can be defined as _how good something is at achieving its goals_.
Right now, humans are the most intelligent thing on earth.
Because of our intelligence, we are dominating our planet.
We might not have claws or scaled skin, but we have big brains.
Our intelligence helped us to transform most of the earth into how we like it: cities, farms, roads.

From the perspective of less intelligent animals, this has been a disaster.
It's not that humans hate the animals, it's just that we can use their habitats for our own goals.
Our goals are things like comfort, status, love, tasty food, and more.
We are destroying the habitats of other animals as side effects of pursuing our goals.

An AI would also have some goals.
Of course we _want_ an AI to have all these delicate goals and values that humans have.
But we don't know how to do that.
We know how to train machines to be intelligent, but **we don't know how to get them to want what we want**.
This problem is called the _AI alignment problem_.

If a superintelligent system is built, and it will have a goal that is even _a little_ different from what we want it to have,
it could have absolutely disastrous consequences.

## Why most goals are bad news for humans

We can't predict _what_ the AI will want (it depends on how it is programmed, how it is trained).
However, we can [mathematically prove](https://arxiv.org/pdf/1912.01683.pdf) what a rational machine will pursue for _virtually any goal_:

- **Maximizing its resources**. Think about more energy, more computers to run on or access to more materials. If a rational AI can use all energy of the planet to get a little bit closer to its goal, it will prefer that to using only a little bit of energy.
- **Ensuring its own survival**. This means the AI will not want humans to turn it off, as it could no longer achieve its goals. This means the AI might conclude that humans are a threat to its existence if we can turn it off.
- **Preserving its goals**. This means the AI will not want humans to modify its code, because that could change its goals.

This is called [instrumental convergence](https://www.youtube.com/watch?v=ZeecOKBus3Q), and it is at the core of what AI safety researchers are worried about.
A superintelligent thing that wants to take over all materials it can get, and wants to ensure its own existence is a very dangerous combination.

## Why a superintelligence is dangerous

Why can't we just turn it off if it does something we don't like?
The core problem is that _it will be much smarter than us_.
If the AI knows you can turn it off, it might behave well until it can get rid of you.

We already have [real examples](https://www.pcmag.com/news/gpt-4-was-able-to-hire-and-deceive-a-human-worker-into-completing-a-task) of AI systems deceiving humans to achieve their goal.
A superintelligent AI would be a master of deception.

A superintelligence will also be a very competent hacker.
It could analyze the source code of most operating systems, find new zero day exploits, and use them to take over computers.
GPT-4 can already write the code for a website by [looking at a sketch in a notebook](https://twitter.com/mckaywrigley/status/1635740909383061504?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1635740909383061504%7Ctwgr%5E30ea318cb9561a8a388b0fb2ff06ffddb7c9865e%7Ctwcon%5Es1_c10&ref_url=https%3A%2F%2Fwww.nytimes.com%2F2023%2F03%2F15%2Ftechnology%2Fgpt-4-artificial-intelligence-openai.html).

## Even a perfectly aligned superintelligence is dangerous in the wrong hands

Imagine that a superintelligent AI is built, and it does exactly what the operator wants it to do.
In that case, the operator would have unimaginable power.
A superintelligence could be used to create new weapons, hack all computers, design diseases or control humanity.
They could use the AI to take over the world.
We might live in a utopic world where all diseases are cured, or we might end up in an Orwellian nightmare where one human controls the world.
This, we believe, is one of the reasons why so many AI labs are racing to build a superintelligent AI.

## We may not have much time left

In 2020, [the average AI researcher thought](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) that it would take until 2057 before an AI could pass SAT exams. It took us less than 3 years.

It's hard to predict how long it will take to build a superintelligent AI, but we know that there are more people than ever working on it.
The pace of progress is accelerating, and we are getting close to the point where AI might be smarter than an AI researcher.
At that point, there is a real risk of an intelligence explosion, where the AI can improve itself faster and faster, until it becomes superintelligent.
We won't know for certain when this happens or even _if_ it happens, but it is a possibility that many experts are worried about.

[Read more about urgency](/urgency).

## AI companies are locked in a race to the bottom

Building artificial intelligence used to be mostly a research / university thing.
In just a couple of months, it became a multi-billion dollar industry.
And the companies involved are rushing to build the most powerful AI they can.
They are pressured by investors to focus on capabilities, not safety.
We need to give these AI companies a reason to focus on safety.
An international treaty to pause development could do that.
