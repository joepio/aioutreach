---
title: Learn about AI alignment
description: Educational resources (videos, articles, books) about AI alignment
---

One of the most important things you can do to help with AI alignment and the existential risk that superintelligence poses, is to learn about it.
Here are some resources to get you started.

## Videos

- [How to get Empowered, not overpowered by AI](https://www.youtube.com/watch?v=2LRwvU6gEbA) (15 mins). A brief introduction to the importance of getting AI alignment right.
- [Robert Miles' YouTube videos](https://www.youtube.com/watch?v=nKJlF-olKmglist=PLqL14ZxTTA4fyhYg6xD6Fz05WcuxLGseL) are a great place to start understanding most of the fundamentals of AI alignment. If you want to really learn how AI aligment works, start here!
- [Max Tegmark with Lex interview](https://youtu.be/VcVfceTsD0A?t=1547) (2 hrs). Interview that dives into the details of our current dangerous situation. _"It's like 'Don't look up', but we are building the asteroid ourselves."_
- [The AI Dilemma](https://www.youtube.com/watch?v=xoVJKj8lcNQ&t=1903s) (1hr). Presentation about the dangers of AI and the race which AI companies are stuck in.
- [How not to destroy the world with AI](https://www.youtube.com/watch?v=ISkAkiAkK7A) (1hr). Presentation by Stuart Russell.

## Websites

- [AISafety.info](https://aisafety.info/) is an absolutely amazing database of questions and answers.

## Podcasts
- [AI X-Risk Research podcast](https://axrp.net/). In-depth interviews with experts in the field of AI alignment.
- [Future of Life podcast](https://soundcloud.com/futureoflife)


## Articles

- [The 'Don't Look Up' Thinking That Could Doom Us With AI](https://time.com/6273743/thinking-that-could-doom-us-with-ai/)
- [Pausing AI Developments Isn't Enough. We Need to Shut it All Down](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/)
## Courses

- [AGI safety fundamentals](https://www.agisafetyfundamentals.com/) (30hrs)
- [A simple explanation of why advanced AI could be incredibly dangerous](https://muddyclothes.substack.com/p/a-simple-explanation-of-why-advanced)

## Organisations

- [Future of Life Institute](https://futureoflife.org/cause-area/artificial-intelligence/) started the [open letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)
- [Effective Altruism](https://www.effectivealtruism.org/). A community of people, many of which are highly concerned about AI safety. They also have a great [post on pursuing a career](https://forum.effectivealtruism.org/posts/7WXPkpqKGKewAymJf/how-to-pursue-a-career-in-technical-ai-alignment) in AI alignment.
